---
title: "Kinetochore-microtubule error correction"
output:
  html_notebook:
    css: nice_notebook.css
---


```{r setup, include=FALSE}
library(knitr)
library(ggplot2)
options(width = 100)

opts_chunk$set(
  warning=FALSE,
  message=FALSE,
  include=FALSE
)
```


# 14/06/2018

Thinking about how to build a model. Started with a perhaps over-complicated model with separate objects: spindles, microtubules and kinetochores.

# 15/06/2018

Finessing the model. Decided to simplify it. Now I only keep kinetochores, each of them in a different attachment states. Then there are rulses of transformation between states. There is no time step, event times are generted from the exponential distribution.

Problems with the alternative model. Crashes on lateral attachment.

# 18/06/2018

Writing document, creating figures. Developing the code and testing. There are issues. Needs more testing.

# 19/06/2018

Testing model - extracting event duration data and comparing to input parameters. Creating snakemake file and running large-scale simulations on the cluster. Running model on a grid of parameters. Still needs testing. For example, detachment is a bit shorter than it should be.

# 20/06/2018

Doing some code tweaking. Discovered package "microbenchmark". Very usesful. For example, it turns out tests like

```
stopifnot(side %in% SIDES)
stopifnot(KT$contact %in% CONTACTS)
```

that I have in some of the functions increase execution time of the entire function by factor two! Commenting them out. Also accessing a field in object (e.g. `KT$contact`) takes time. So, if I have `if(KT$contact == ...)` multiple times, it is faster to create a new variable `contact` and test it.

Tests look fine, though, for some reason, detachment is a bit faster in simulations that it should be. It's a tiny difference, but there is one. Creating plots of test distributions.

Since there is very little dependence on formation and detachment rate, I decided to widen the range of parameters. To avoide huge calculations and huge objects, I reduced the number of simulations from 100,000 to 10,000 per set of parameters. This should be enough for mean or median time.

# 21/06/2018

There is still a small discrepancy between event durations and theoretical exponential distributions. It's bugging me.

A code modification for better testing: now each generated event (event, duraion) is stored. So, I can recover these events later and check duration distribution directly. Still, there are discrepancies. I also tried changing random seed before running tests. No effect.

New figures: graphical representation of a simulation's timeline.

# 22/06/2018

Renaming models, re-running simulations. Adding more description to the document.

Decided to separete the models and run the grid for each model separetely. Now the results is different: detachment rate became important for model M1. This is because when I did models together, it was diluted by model M2.


Meeting with Tomo: need to change parameter ranges (they were incorrect). Also, the final stable configuration can be on of the two:

```
endon-L endon-R
endon-R endon-L
```
